{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import plotly.io as pio\n",
    "import charset_normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from typing import Any, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory: str, file_format: str, exclude_substring: str | list = None):\n",
    "    \"\"\"\n",
    "    Locate all files of a certain format in a folder and its subfolders.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The directory to search in.\n",
    "    - file_format (str): The file format to search for (e.g., '.txt').\n",
    "    - exclude_substring (str): A substring to include if files containing it is to be excluded\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of paths to the files found.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, f\"*{file_format}\"):\n",
    "            if not any(substring in filename for substring in exclude_substring):\n",
    "                matches.append(os.path.join(root, filename))\n",
    "    return matches\n",
    "\n",
    "\n",
    "def load_and_append_data(file_list: list, date_column: str, file_headers: list):\n",
    "    # Initialize an empty list to hold DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            # Load the .txt file into a DataFrame\n",
    "            new_df = pd.read_csv(file_path, sep=',', names=file_headers)  # Change sep if necessary\n",
    "\n",
    "            # Check if the date column exists in the new DataFrame\n",
    "            if date_column not in new_df.columns:\n",
    "                raise ValueError(f\"The specified date column '{date_column}' is not in the DataFrame from '{file_path}'.\")\n",
    "\n",
    "            # Convert the date column to datetime format for accurate sorting\n",
    "            new_df[date_column] = pd.to_datetime(new_df[date_column], format='%d/%m/%Y %H:%M:%S', dayfirst=True)\n",
    "\n",
    "            # Sort the new DataFrame by the date column\n",
    "            new_df = new_df.sort_values(by=date_column)\n",
    "\n",
    "            # Append sorted DataFrame to the list\n",
    "            all_dfs.append(new_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing '{file_path}': {e}\")\n",
    "            continue  # Skip to the next file if there's an error\n",
    "\n",
    "    # Combine all the DataFrames into one\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Sort the combined DataFrame by the date column\n",
    "    combined_df = combined_df.sort_values(by=date_column)\n",
    "\n",
    "    # Remove duplicates based on all columns or specify certain columns\n",
    "    combined_df = combined_df.drop_duplicates(subset=[date_column])\n",
    "\n",
    "    # Reset index after dropping duplicates and drop the old index\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Make date_column into a date formatted column\n",
    "    combined_df[date_column] = pd.to_datetime(combined_df[date_column], dayfirst=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def resample_dataframe(df: pd.DataFrame, resample_interval: str = '5 min', date_column: str = 'date_time'):\n",
    "    \"\"\"\n",
    "    Resample a DataFrame containing 1-minute data to 5-minute intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 1-minute data with a datetime column.\n",
    "    - date_column: Name of the column that contains datetime information.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame resampled to 5-minute intervals.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    type_check = False\n",
    "    df_temp = df.copy()\n",
    "    df_temp[date_column] = pd.to_datetime(df_temp[date_column])\n",
    "\n",
    "    # Set the date column as the index\n",
    "    df_temp.set_index(date_column, inplace=True)\n",
    "    try:\n",
    "        df_temp.drop('Type', axis=1, inplace=True)\n",
    "    except KeyError:\n",
    "        type_check = True\n",
    "    # Resample the data to 5-minute intervals and aggregate using mean\n",
    "    resampled_df = df_temp.resample(f'{resample_interval}').mean().round(2)\n",
    "\n",
    "    # Reset the index to have the date column back as a column\n",
    "    resampled_df.reset_index(inplace=True, names='Date')\n",
    "    if type_check:\n",
    "        resampled_df['Type'] = f'{resample_interval} average'\n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observation:\n",
    "    def __init__(self, \n",
    "                 species_name: str, \n",
    "                 measurements: List[Any, float | np.float64] | np.ndarray[Any, float | np.float64] | pd.DataFrame, \n",
    "                 unit: str) -> None:\n",
    "        \n",
    "        self.species_name = species_name\n",
    "        self.unit = unit\n",
    "\n",
    "        if type(measurements) == np.ndarray:\n",
    "            measurements_placeholder = pd.DataFrame(measurements)\n",
    "        elif type(measurements) == list:\n",
    "            measurements_placeholder =  pd.DataFrame(measurements)\n",
    "        else:\n",
    "            raise ValueError('The format of the measurements are not of an accepted format (list, np.ndarray or pd.DataFrame).')\n",
    "        \n",
    "        try:\n",
    "            measurements_placeholder.iloc[:, 0].astype(float)\n",
    "            measurements_placeholder.columns.values[0] = f'{species_name} [{unit}]'\n",
    "            measurements_placeholder.columns.values[1] = 'Date'\n",
    "        except ValueError:\n",
    "            measurements_placeholder.columns.values[0] = 'Date'\n",
    "            measurements_placeholder.columns.values[1] = f'{species_name} [{unit}]'\n",
    "        \n",
    "        try:\n",
    "            measurements_placeholder['Date'] = pd.to_datetime(measurements_placeholder['Date'])\n",
    "        except ValueError:\n",
    "            raise ValueError('A timeseries for the measurements were not found in the data.')\n",
    "        self.measurements = measurements_placeholder\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'This is measurements of {self.species_name} with {len(self.measurements)} measurements.'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class StoredObservation(Observation):\n",
    "    def __init__(self, \n",
    "                 species_name: str, \n",
    "                 measurements: List[Any, float | np.float64] | np.ndarray[Any, float | np.float64] | pd.DataFrame, \n",
    "                 unit: str) -> None:\n",
    "        super().__init__(species_name, measurements, unit)\n",
    "\n",
    "    \n",
    "    def compare_to_original(self, original: 'Observation') -> str:\n",
    "        if self.species_name != original.species_name:\n",
    "            raise TypeError('The samples to be compared are of two different species.')\n",
    "        elif self.unit != original.unit:\n",
    "            raise ValueError('The units of these observations are different from the original.')\n",
    "        elif self.measurements['Date'] != original.measurements['Date']:\n",
    "            raise ValueError('The the observations are of two different timeperiods.')\n",
    "        else:\n",
    "            if self.measurements[f'{self.species_name} [{self.unit}]'] != original.measurements[f'{original.species_name} [{original.unit}]']:\n",
    "                print('The two observations are identical.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 'aeth_data_raw/2021/AE33_AE33-S05-00497_20210114.dat'\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the header using the ';' separator\n",
    "header_df = pd.read_csv(l, sep=';', skiprows=5).columns\n",
    "drop_headers = ['drop1', 'drop2', 'drop3', 'drop4', 'drop5']\n",
    "header_df = [*header_df[:-1], *drop_headers]\n",
    "# Display the header to inspect it\n",
    "\n",
    "# Step 2: Load the actual data using the space separator, skipping the header rows\n",
    "data_df = pd.read_csv(l, sep=' ', skiprows=6 , names=header_df)\n",
    "\n",
    "# Display the first few rows of the data to inspect it\n",
    "print(\"\\nData DataFrame:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# Step 3: Clean the data by dropping any unassigned columns (entirely NaN columns)\n",
    "#data_df = data_df.dropna(axis=1, how='all')  # Drop columns that are entirely NaN\n",
    "\n",
    "# Optionally, if you know the names or indices of unassigned columns, you can drop them directly\n",
    "# Example: data_df = data_df.drop(columns=['Unassigned1', 'Unassigned2'])\n",
    "\n",
    "# Display the cleaned DataFrame to confirm changes\n",
    "print(\"\\nCleaned Data DataFrame:\")\n",
    "print(data_df.head())\n",
    "\n",
    "# Step 4: Save the cleaned DataFrame to a new CSV file\n",
    "#data_df.to_csv('path/to/your/cleaned_file.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned data has been saved to 'path/to/your/cleaned_file.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading and processing files: 100%|██████████| 2558/2558 [41:47<00:00,  1.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"aeth_data_raw/\"  # Replace with your folder path\n",
    "    file_extension = \".dat\"    # Replace with your desired file format\n",
    "    file_exclusion = ['log', 'CT', 'ST']\n",
    "    drop_headers = ['drop1', 'drop2', 'drop3', 'drop4', 'drop5']\n",
    "    found_files = find_files(folder_path, file_extension, exclude_substring=file_exclusion)\n",
    "\n",
    "    df_list = []\n",
    "    for file in tqdm(found_files[87:], desc=\"Loading and processing files\"):\n",
    "        #with open(file, 'rb') as f:\n",
    "        #    result = charset_normalizer.detect(f.read())\n",
    "        try:\n",
    "            header_df = pd.read_csv(file, sep=';', skiprows=5).columns#, encoding=result['encoding']).columns\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        header_df = [*header_df[:-1], *drop_headers]\n",
    "\n",
    "        temp = pd.read_csv(file, sep=' ', skiprows=6 , names=header_df, encoding=result['encoding'])\n",
    "        temp.drop(drop_headers, axis=1, inplace=True)\n",
    "        df_list.append(temp)\n",
    "    aeth_df = pd.concat(df_list)\n",
    "    aeth_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    ### How many significant digits in the final BC concentrations? ###\n",
    "    aeth_df['BC_final'] = (aeth_df[' BC6'] / 1.6121).round(3)\n",
    "    aeth_BC_final = aeth_df.loc[:, ['BC_final', \n",
    "                                    ' BB(%)', \n",
    "                                    ' Pressure(Pa)', \n",
    "                                    ' Temperature(°C)', \n",
    "                                    ' Flow1',\n",
    "                                    ' Flow2',\n",
    "                                    ' Status', \n",
    "                                    ' TapeAdvCount']]\n",
    "    \n",
    "    aeth_BC_final['Date'] = pd.to_datetime(aeth_df['Date(yyyy/MM/dd)'].astype(str) + ' ' + \n",
    "                                           aeth_df[' Time(hh:mm:ss)'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b><u>Manual Zero and Calibration periods</b></u>\n",
    "\n",
    "<b><u>Manual Calibration</b></u>\n",
    "<b><i>2016</b></i>\n",
    "From: 19/08 - 2016 12:01\n",
    "To:   19/08 - 2016 14:24\n",
    "\n",
    "From: 19/08 - 2016 15:54\n",
    "To:   19/08 - 2016 16:42\n",
    "________________________________________________\n",
    "<b><i>2018</b></i>\n",
    "From: 23/03 - 2018 11:23\n",
    "To:   23/03 - 2018 15:07\n",
    "________________________________________________\n",
    "<b><i>2022</b></i>\n",
    "From: 14/12 - 2022 15:50\n",
    "To:   15/12 - 2022 12:40\n",
    "________________________________________________\n",
    "<b><i>2023</b></i>\n",
    "From: 17/04 - 2023 19:57\n",
    "To:   17/04 - 2023 21:27\n",
    "________________________________________________\n",
    "<b><i>2024</b></i>\n",
    "From: 23/03 - 2024 13:00\n",
    "To:   23/03 - 2024 14:50\n",
    "________________________________________________\n",
    "\n",
    "<b><u>Manual Zero</b></u>\n",
    "<b><i>2016</b></i>\n",
    "<u>Before</u>\n",
    "From: 19/08 - 2016 10:26\n",
    "To:   19/08 - 2016 11:58\n",
    "\n",
    "<u>After</u>\n",
    "From: 19/08 - 2016 14:27\n",
    "To:   19/08 - 2016 15:44\n",
    "________________________________________________\n",
    "<b><i>2018</b></i>\n",
    "<u>Before</u>\n",
    "From: 22/03 - 2018 16:14\n",
    "To:   23/03 - 2018 10:59\n",
    "\n",
    "<u>After</u>\n",
    "<s>From: dd/mm - yyyy HH:MM</s>\n",
    "<s>To:   dd/mm - yyyy HH:MM</s>\n",
    "________________________________________________\n",
    "<b><i>2022</b></i>\n",
    "<u>Before</u>\n",
    "From: 11/12 - 2022 13:05\n",
    "To:   14/12 - 2022 12:05\n",
    "\n",
    "<u>After</u>\n",
    "From: 15/12 - 2022 15:10\n",
    "To:   16/12 - 2022 08:35\n",
    "________________________________________________\n",
    "<b><i>2023</b></i>\n",
    "<u>Before</u>\n",
    "From: 17/04 - 2023 17:57\n",
    "To:   17/04 - 2023 19:23\n",
    "\n",
    "<u>After</u>\n",
    "From: 17/04 - 2023 21:30\n",
    "To:   18/04 - 2023 09:45\n",
    "________________________________________________\n",
    "<b><i>2024</b></i>\n",
    "<u>Before</u> \n",
    "From: 22/03 - 2024 16:50\n",
    "To:   23/03 - 2024 08:35\n",
    "\n",
    "<u>After</u>\n",
    "From: 23/03 - 2024 15:25\n",
    "To:   24/03 - 2024 11:50\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Nephelometer data and load as a Pandas DataFrame ###\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of .txt files to load, including the existing data file\n",
    "    folder_path = \"neph_data_raw/\" \n",
    "    file_extension = \".txt\"  \n",
    "    neph_headers = ['Date', \n",
    "                    'Type', \n",
    "                    'Scat635nm [Mm^-1]', \n",
    "                    'Scat525nm [Mm^-1]', \n",
    "                    'Scat450nm [Mm^-1]', \n",
    "                    'BackScat635nm [Mm^-1]', \n",
    "                    'BackScat525nm [Mm^-1]', \n",
    "                    'BackScat450nm [Mm^-1]', \n",
    "                    'Sample Temp. [K]', \n",
    "                    'Enclosure Temp. [K]', \n",
    "                    'RH [%]', \n",
    "                    'Pressure [mBar]'\n",
    "    ]\n",
    "\n",
    "    file_list = find_files(folder_path, file_extension, exclude_substring=[])\n",
    "    \n",
    "    # Specify the date column name (ensure it's the same in all DataFrames)\n",
    "    date_column = 'Date' \n",
    "\n",
    "    # Load, sort, and append the new data\n",
    "    neph_df = load_and_append_data(file_list, date_column, neph_headers)\n",
    "    neph_df = neph_df[neph_df['Date'] >= pd.to_datetime('2016/01/01 00:00:00', dayfirst=True)]\n",
    "    # Optionally, save the updated DataFrame to a new CSV file\n",
    "    #if updated_df is not None:\n",
    "    #    updated_df.to_csv('path/to/your/updated_file.csv', index=False)  # Update this path\n",
    "    #    print(\"Data has been successfully updated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract all one minute values ###\n",
    "one_min_instant = neph_df[\n",
    "    (neph_df['Date'] >= pd.to_datetime('18/04/2023 09:46:00', dayfirst=True)) & \n",
    "    (neph_df['Date'] <= pd.to_datetime('25/03/2024 09:59:00', dayfirst=True))]\n",
    "\n",
    "one_min_instant.set_index('Date', inplace=True)\n",
    "one_min_instant = one_min_instant.reindex(pd.date_range(start=pd.to_datetime('18/04/2023 09:46:00', dayfirst=True), end=pd.to_datetime('25/03/2024 09:59:00', dayfirst=True), freq='1min', inclusive='both'))\n",
    "one_min_instant.reset_index(inplace=True, names='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Resample the DataFrame to 5-minute intervals\n",
    "    header_order = one_min_instant.columns\n",
    "    df_5min = resample_dataframe(one_min_instant[one_min_instant['Type'] == '1 min instant'], date_column='Date')\n",
    "    df_5min = df_5min[header_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_calibration_times = {'2016_1': ['19-08-2016 12:01', '19-08-2016 14:24'], \n",
    "                         '2016_2': ['19-08-2016 15:54', '19-08-2016 16:42'], \n",
    "                         '2018': ['23-03-2018 11:23', '23-03-2018 15:07'],\n",
    "                         '2022': ['14-12-2022 15:50', '15-12-2022 12:40'],\n",
    "                         '2023': ['17-04-2023 19:57', '17-04-2023 21:27'],\n",
    "                         '2024': ['23-03-2024 13:00', '23-03-2024 14:50']}\n",
    "\n",
    "manual_zero_times = {'2016_before': ['19-08-2016 10:26', '19-08-2016 11:58'],\n",
    "                     '2016_after': ['19-08-2016 14:27', '19-08-2016 15:44'],\n",
    "                     '2018_before': ['22-03-2018 16:14', '23-03-2018 10:59'],\n",
    "                     '2022_before': ['11-12-2022 13:05', '14-12-2022 12:05'],\n",
    "                     '2022_after': ['15-12-2022 15:10', '16-12-2022 08:35'],\n",
    "                     '2023_before': ['17-04-2023 17:57', '17-04-2023 19:23'],\n",
    "                     '2023_after': ['17-04-2023 21:30', '18-04-2023 09:45'],\n",
    "                     '2024_before': ['22-03-2024 16:50', '23-03-2024 08:35'],\n",
    "                     '2024_after': ['23-03-2024 15:25', '24-03-2024 11:50']\n",
    "                     }\n",
    "# Locate and slice out all data in one minute instant format\n",
    "neph_df_sliced = neph_df[\n",
    "    (neph_df['Date'] < pd.to_datetime('18/04/2023 09:46:00', dayfirst=True)) | \n",
    "    (neph_df['Date'] > pd.to_datetime('25/03/2024 09:59:00', dayfirst=True))]\n",
    "\n",
    "# Add back the one minute instant formatted data in a 5 minute average, so the data is consistent\n",
    "neph_df_5_min = pd.concat([neph_df_sliced, df_5min])\n",
    "neph_df_5_min.sort_values(by='Date', inplace=True)\n",
    "neph_df_5_min.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Sort the different types of data in separate dataframes for handling\n",
    "# Measurement data\n",
    "neph_data = neph_df_5_min[neph_df_5_min['Type'] == '5 min average'].reset_index(drop=True)\n",
    "\n",
    "# Possible Manual Zero and calibration data\n",
    "neph_zero_cal = neph_df_5_min[neph_df_5_min['Type'] == '1 min instant'].reset_index(drop=True)\n",
    "\n",
    "# All automated zeroes\n",
    "neph_auto_zero = neph_df_5_min[\n",
    "    (neph_df_5_min['Type'] == 'Zero 5 min instant') | \n",
    "    (neph_df_5_min['Type'] == 'Zero check final')].reset_index(drop=True)\n",
    "\n",
    "# Remove period after automated zero application was turned on\n",
    "neph_auto_zero = neph_auto_zero[neph_auto_zero['Date'] <= pd.to_datetime('18/04/2023 09:46:00', dayfirst=True)].reset_index(drop=True)\n",
    "\n",
    "# All automated zeroes in the one minute instant format\n",
    "neph_auto_zero_cal_1min = one_min_instant[\n",
    "    (one_min_instant['Type'] == 'Zero 1 min instant') | \n",
    "    (one_min_instant['Type'] == 'Zero check final') | \n",
    "    (one_min_instant['Type'] == 'Span 1 min instant')].reset_index(drop=True)\n",
    "\n",
    "# Remove period after automated zero application was turned on\n",
    "neph_span_1min = neph_auto_zero_cal_1min[neph_auto_zero_cal_1min['Type']=='Span 1 min instant'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataframe(df, column_name=None, split_string=None, date_column=None, minute_diff=5):\n",
    "    # List to hold the new DataFrames\n",
    "    dataframes = {}\n",
    "    \n",
    "    # Find the indices where the split string appears\n",
    "    if split_string is not None:\n",
    "        try:\n",
    "            indices = df.index[df[column_name].str.contains(split_string, na=False)].tolist()\n",
    "        except ValueError:\n",
    "            raise ValueError('Please input a name for the column to be used for the split.')\n",
    "    elif date_column is not None:\n",
    "        indices = []\n",
    "        for i, value in enumerate(df[date_column]):\n",
    "            if ((pd.to_datetime(value) - pd.to_datetime(df[date_column].iloc[i-1])).total_seconds() / 60) > minute_diff: \n",
    "                indices.append(i-1)\n",
    "            else:\n",
    "                continue\n",
    "        indices.append(len(df))\n",
    "    else:\n",
    "        print('Please specify a date column for the DataFrame.')\n",
    "    # Slice the DataFrame into smaller DataFrames based on the indices\n",
    "    for i in range(len(indices)):\n",
    "        if i == 0:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = indices[i-1]+1  # Start after the split string\n",
    "        end = indices[i]+1 if i+1 < len(indices) else len(df)  # End at the next split or the end of the DataFrame\n",
    "        # Create a new DataFrame slice\n",
    "        new_df = df.iloc[start:end].reset_index(drop=True)\n",
    "        dataframes[f'{new_df['Date'].iloc[0]}'] = new_df\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def apply_correction(data_to_correct, correction_data, correction_type):\n",
    "    \n",
    "    temp_df = data_to_correct.copy()\n",
    "    # Step 1: Iterate through the rows of df_zero to subtract the zero measurements\n",
    "    for idx, row in correction_data.iterrows():\n",
    "        correction_time = pd.to_datetime(row['Date'])  # The timestamp when the zero measurement should apply'\n",
    "\n",
    "        # Get the next zero timestamp or the last time\n",
    "        next_correction_time = pd.to_datetime(correction_data['Date'].iloc[idx+1] if idx + 1 < len(correction_data) else temp_df.index[-1])\n",
    "\n",
    "        # Get the columns that match\n",
    "        matching_columns = [col for col in temp_df.columns if col in row.index and col != 'Date']\n",
    "\n",
    "        # Step 2: Apply the zero measurement correction for the entire period\n",
    "        for col in matching_columns:\n",
    "            if correction_type == '-':\n",
    "                # Subtract the zero measurement (with noise) from all rows\n",
    "                temp_df.loc[(temp_df['Date'] >= correction_time) & \n",
    "                            (temp_df['Date'] < next_correction_time), col] -= row[col]\n",
    "            elif correction_type == '+':\n",
    "                temp_df.loc[(temp_df['Date'] >= correction_time) & \n",
    "                            (temp_df['Date'] < next_correction_time), col] += row[col]\n",
    "            elif correction_type == '*':\n",
    "                temp_df.loc[(temp_df['Date'] >= correction_time) & \n",
    "                            (temp_df['Date'] < next_correction_time), col] *= row[col]\n",
    "            elif correction_type == '/':\n",
    "                temp_df.loc[(temp_df['Date'] >= correction_time) & \n",
    "                            (temp_df['Date'] < next_correction_time), col] /= row[col]\n",
    "            else:\n",
    "                raise ValueError(f'Correction type {correction_type} not recognized, please specify one of the following: +, -, *, /.')\n",
    "    temp_df[matching_columns] = temp_df[matching_columns].round(2)\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def merge_monthly_data(data):\n",
    "    \"\"\"\n",
    "    Merges entries with the same year and month by averaging their values (which are pandas Series),\n",
    "    keeping the earliest timestamp as the key.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Ordered dictionary where keys are timestamps (as strings)\n",
    "                     and values are pandas Series.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated dictionary with merged values for timestamps in the same month and year.\n",
    "    \"\"\"\n",
    "    # Convert the dictionary keys to datetime for easier comparison\n",
    "    data = {pd.to_datetime(k): v for k, v in data.items()}\n",
    "\n",
    "    # Output dictionary to store the result\n",
    "    output = {}\n",
    "\n",
    "    # Iterate over the dictionary, checking adjacent keys\n",
    "    keys_to_remove = []  # To track keys that are merged\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        timestamp, values = list(data.items())[i]\n",
    "        prev_timestamp, prev_values = list(data.items())[i - 1]\n",
    "\n",
    "        # Check if both timestamps are in the same month and year\n",
    "        if timestamp.year == prev_timestamp.year and timestamp.month == prev_timestamp.month:\n",
    "            # Calculate the mean of the corresponding Series objects\n",
    "            mean_values = (prev_values + values) / 2\n",
    "            \n",
    "            # Keep the earlier timestamp and store the mean values\n",
    "            output[str(prev_timestamp)] = mean_values.round(2)\n",
    "            keys_to_remove.append(timestamp)\n",
    "        else:\n",
    "            # If timestamps are not in the same month/year, just copy the current row\n",
    "            output[str(prev_timestamp)] = prev_values\n",
    "\n",
    "    # Add the last entry if not merged\n",
    "    last_timestamp, last_values = list(data.items())[-1]\n",
    "    if last_timestamp not in keys_to_remove:\n",
    "        output[str(last_timestamp)] = last_values\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    co2_full_scatter = {'635nm': 11.15,\n",
    "                        '525nm': 23.86,\n",
    "                        '450nm': 44.21}\n",
    "\n",
    "    co2_back_scatter = {'635nm': 5.58,\n",
    "                        '525nm': 11.93,\n",
    "                        '450nm': 22.11}\n",
    "\n",
    "    full_headers = [head for head in neph_auto_zero.columns if 'Scat' in head and 'Back' not in head]\n",
    "    back_headers = [head for head in neph_auto_zero.columns if 'Scat' in head and 'Back' in head]\n",
    "\n",
    "    # Find all periods where automated zero measurements have taken place\n",
    "    neph_auto_zero_dict = slice_dataframe(neph_auto_zero, 'Type', 'Zero check final')\n",
    "    # neph_auto_zero_1min_dict = slice_dataframe(neph_auto_zero_cal_1min[neph_auto_zero_cal_1min['Type'] != 'Span 1 min instant'], date_column='Date', minute_diff=1)\n",
    "\n",
    "    neph_man_zeroes = {}\n",
    "    neph_auto_zeroes = {}\n",
    "    calibration_dict = {}\n",
    "    neph_co2_calibration = {}\n",
    "    neph_auto_zeroes_1min = {}\n",
    "\n",
    "    for key, times in manual_zero_times.items():\n",
    "        tt = [pd.to_datetime(time, dayfirst=True) for time in times]\n",
    "        try:\n",
    "            neph_man_zeroes[str(tt[0])] = neph_zero_cal.iloc[neph_zero_cal[neph_zero_cal['Date']==tt[0]].index[0]:\n",
    "                                                             neph_zero_cal[neph_zero_cal['Date']==tt[1]].index[0]+1, 2:].mean().round(2).T\n",
    "        except IndexError:\n",
    "            neph_man_zeroes[str(tt[0])] = neph_df_5_min.iloc[neph_df_5_min[neph_df_5_min['Date']==tt[0]].index[0]:\n",
    "                                                             neph_df_5_min[neph_df_5_min['Date']==tt[1]].index[0]+1, 2:].mean().round(2).T\n",
    "            \n",
    "            neph_df_5_min_data = neph_df_5_min.drop(neph_df_5_min.index[neph_df_5_min[neph_df_5_min['Date']==tt[0]].index[0]:\n",
    "                                                                        neph_df_5_min[neph_df_5_min['Date']==tt[1]].index[0]+1])\n",
    "            \n",
    "    for key, df in neph_auto_zero_dict.items():\n",
    "        neph_auto_zeroes[key] = df.iloc[:, 2:].mean().round(2).T\n",
    "\n",
    "    # for key, df in neph_auto_zero_1min_dict.items():\n",
    "    #     neph_auto_zeroes_1min[key] = df.iloc[:, 2:].mean().round(2).T\n",
    "    #     #print(key, neph_auto_zeroes_1min[key])\n",
    "    \n",
    "    avg_man_zeroes = merge_monthly_data(neph_man_zeroes)\n",
    "\n",
    "    neph_all_zeroes = {**neph_auto_zeroes, **neph_auto_zeroes_1min, **avg_man_zeroes}\n",
    "    neph_all_zeroes = dict(sorted(neph_all_zeroes.items()))\n",
    "\n",
    "    neph_zeroes_df = pd.DataFrame(neph_all_zeroes).T  # Transpose to make dates as rows\n",
    "\n",
    "    # Add 'Date' column with the date as a column, reset index to make it part of the DataFrame\n",
    "    neph_zeroes_df['Date'] = pd.to_datetime(neph_zeroes_df.index)\n",
    "    neph_zeroes_df = neph_zeroes_df.drop(['Sample Temp. [K]', 'Enclosure Temp. [K]', 'RH [%]', 'Pressure [mBar]'], axis=1).reset_index(drop=True)  # Reset index to get rid of the original index\n",
    "\n",
    "    # Reorder the columns to have 'Date' first\n",
    "    neph_zeroes_df = neph_zeroes_df[['Date'] + [col for col in neph_zeroes_df.columns if col != 'Date']]\n",
    "\n",
    "    neph_zeroed_cal_data = apply_correction(neph_zero_cal, neph_zeroes_df, '-')\n",
    "    neph_zeroed_data = apply_correction(neph_data, neph_zeroes_df, '-')\n",
    "\n",
    "    for key, times in co2_calibration_times.items():\n",
    "        tl = [pd.to_datetime(time, dayfirst=True) for time in times]\n",
    "        try:\n",
    "            neph_co2_calibration[str(tl[0])] = neph_zeroed_cal_data.iloc[neph_zeroed_cal_data[neph_zeroed_cal_data['Date']==tl[0]].index[0]:\n",
    "                                                                         neph_zeroed_cal_data[neph_zeroed_cal_data['Date']==tl[1]].index[0]+1, 2:]\n",
    "        except IndexError:\n",
    "            neph_co2_calibration[str(tl[0])] = neph_df_5_min_data.iloc[neph_df_5_min_data[neph_df_5_min_data['Date']==tl[0]].index[0]:\n",
    "                                                                       neph_df_5_min_data[neph_df_5_min_data['Date']==tl[1]].index[0]+1, 2:]\n",
    "        calibration_dict_full = {}\n",
    "        for wl, item in co2_full_scatter.items():\n",
    "            #print(tl)\n",
    "            corr_item = item * (neph_co2_calibration[str(tl[0])]['Pressure [mBar]']/1013.25) * (273.15/293.15)\n",
    "            for head in full_headers:\n",
    "                if wl in head:\n",
    "                    calibration_dict_full[head] = [round((corr_item/neph_co2_calibration[str(tl[0])][head]).mean(), 2)]\n",
    "\n",
    "        calibration_dict_back = {}\n",
    "        for wl, item in co2_back_scatter.items():\n",
    "            corr_item = item * (neph_co2_calibration[str(tl[0])]['Pressure [mBar]']/1013.25) * (273.15/293.15) \n",
    "            for head in back_headers:\n",
    "                if wl in head:\n",
    "                    calibration_dict_back[head] = [round((corr_item/neph_co2_calibration[str(tl[0])][head]).mean(), 2)]\n",
    "        \n",
    "        temp = {**calibration_dict_full, **calibration_dict_back}\n",
    "        \n",
    "        if not calibration_dict:\n",
    "            for key in temp:\n",
    "                calibration_dict[key] = temp[key]\n",
    "        else:\n",
    "            for key in calibration_dict:\n",
    "                calibration_dict[key].extend(temp[key])\n",
    "\n",
    "\n",
    "    calibration_coefficients = pd.DataFrame(calibration_dict)\n",
    "    calibration_coefficients['Date'] = pd.to_datetime(list(neph_co2_calibration.keys()), )\n",
    "\n",
    "    neph_zero_cal_data = apply_correction(neph_zeroed_data, calibration_coefficients, '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate the calibration coefficient\n",
    "def interpolate_calibration(timestamp, cal_df, coef_name):  \n",
    "    # Find the two calibrations between which this timestamp falls\n",
    "    for i in range(1, len(cal_df)):\n",
    "        cal_1 = cal_df.iloc[i - 1]\n",
    "        cal_2 = cal_df.iloc[i]\n",
    "        \n",
    "\n",
    "        if cal_1['Date'] <= timestamp <= cal_2['Date']:\n",
    "            if pd.to_datetime('2022-12-14 15:50:00') < timestamp <= pd.to_datetime('2023-04-17 19:57:00'):\n",
    "                return cal_1[coef_name]\n",
    "            # Linear interpolation formula\n",
    "            t1 = cal_1['Date']\n",
    "            t2 = cal_2['Date']\n",
    "            c1 = cal_1[coef_name]\n",
    "            c2 = cal_2[coef_name]\n",
    "            \n",
    "            # Calculate the interpolated calibration coefficient\n",
    "            delta_time = (timestamp - t1).total_seconds()  # Time difference in seconds\n",
    "            delta_cal = c2 - c1  # Difference in calibration coefficients\n",
    "            total_time = (t2 - t1).total_seconds()  # Total time difference in seconds\n",
    "            \n",
    "            # Interpolated coefficient using linear interpolation\n",
    "            return c1 + (delta_time / total_time) * delta_cal\n",
    "\n",
    "# Apply the interpolation to the timeseries data\n",
    "neph_test = neph_zeroed_data.copy()\n",
    "neph_test['Scat635nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'Scat635nm [Mm^-1]'))\n",
    "neph_test['Scat525nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'Scat525nm [Mm^-1]'))\n",
    "neph_test['Scat450nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'Scat450nm [Mm^-1]'))\n",
    "neph_test['BackScat635nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'BackScat635nm [Mm^-1]'))\n",
    "neph_test['BackScat525nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'BackScat525nm [Mm^-1]'))\n",
    "neph_test['BackScat450nm Coeff'] = neph_test['Date'].apply(lambda x: interpolate_calibration(x, calibration_coefficients, 'BackScat450nm [Mm^-1]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "neph_start = pd.to_datetime(neph_test['Date'].iloc[0])\n",
    "neph_end = pd.to_datetime(neph_test['Date'].iloc[-1])\n",
    "neph_test.set_index('Date', inplace=True)\n",
    "neph_test = neph_test.reindex(pd.date_range(start=neph_start, end=neph_end, freq='5min', inclusive='both'))\n",
    "neph_test.reset_index(inplace=True, names='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill only the trailing NaNs (end of the column) with the last valid value\n",
    "def fill_trailing_nans(col):\n",
    "    # Check if the column has NaNs at the end\n",
    "    last_valid_value = col.last_valid_index()  # Get the last valid index\n",
    "\n",
    "    if pd.notna(last_valid_value):  # If there are valid values in the column\n",
    "        # Get the value of the last valid index\n",
    "        last_value = col[last_valid_value]\n",
    "\n",
    "        # Fill NaNs at the end (after the last valid index) with the last value\n",
    "        col[last_valid_value + 1:] = last_value\n",
    "        \n",
    "    return col\n",
    "\n",
    "# Apply the function to each column in the DataFrame\n",
    "neph_test = neph_test.apply(fill_trailing_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "neph_test['Final Scat635nm [Mm^-1]'] = np.where(neph_test['Scat635nm Coeff'].isna(), neph_test['Scat635nm [Mm^-1]'], neph_test['Scat635nm Coeff'] * neph_test['Scat635nm [Mm^-1]'])\n",
    "neph_test['Final Scat525nm [Mm^-1]'] = np.where(neph_test['Scat525nm Coeff'].isna(), neph_test['Scat525nm [Mm^-1]'], neph_test['Scat525nm Coeff'] * neph_test['Scat525nm [Mm^-1]'])\n",
    "neph_test['Final Scat450nm [Mm^-1]'] = np.where(neph_test['Scat450nm Coeff'].isna(), neph_test['Scat450nm [Mm^-1]'], neph_test['Scat450nm Coeff'] * neph_test['Scat450nm [Mm^-1]'])\n",
    "neph_test['Final BackScat635nm [Mm^-1]'] = np.where(neph_test['BackScat635nm Coeff'].isna(), neph_test['BackScat635nm [Mm^-1]'], neph_test['BackScat635nm Coeff'] * neph_test['BackScat635nm [Mm^-1]'])\n",
    "neph_test['Final BackScat525nm [Mm^-1]'] = np.where(neph_test['BackScat525nm Coeff'].isna(), neph_test['BackScat525nm [Mm^-1]'], neph_test['BackScat525nm Coeff'] * neph_test['BackScat525nm [Mm^-1]'])\n",
    "neph_test['Final BackScat450nm [Mm^-1]'] = np.where(neph_test['BackScat450nm Coeff'].isna(), neph_test['BackScat450nm [Mm^-1]'], neph_test['BackScat450nm Coeff'] * neph_test['BackScat450nm [Mm^-1]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "neph_test.to_csv('neph_data_corrected/full_nephelometer_corrected_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 8))\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['Scat635nm [Mm^-1]'], label='True Full 635nm')\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['Scat525nm [Mm^-1]'], label='True Full 525nm')\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['Scat450nm [Mm^-1]'], label='True Full 450nm')\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['BackScat635nm [Mm^-1]'], label='True Back 635nm')\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['BackScat525nm [Mm^-1]'], label='True Back 525nm')\n",
    "ax.plot(calibration_coefficients['Date'], calibration_coefficients['BackScat450nm [Mm^-1]'], label='True Back 450nm')\n",
    "\n",
    "ax.plot(neph_test['Date'], neph_test['Scat635nm Coeff'], label='Interpolated Full 635nm')\n",
    "ax.plot(neph_test['Date'], neph_test['Scat525nm Coeff'], label='Interpolated Full 525nm')\n",
    "ax.plot(neph_test['Date'], neph_test['Scat450nm Coeff'], label='Interpolated Full 450nm')\n",
    "ax.plot(neph_test['Date'], neph_test['BackScat635nm Coeff'], label='Interpolated Back 635nm')\n",
    "ax.plot(neph_test['Date'], neph_test['BackScat525nm Coeff'], label='Interpolated Back 525nm')\n",
    "ax.plot(neph_test['Date'], neph_test['BackScat450nm Coeff'], label='Interpolated Back 450nm')\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "2024-09-05 16:05:00\n"
     ]
    }
   ],
   "source": [
    "start_date = neph_zero_cal_data['Date'].iloc[0]\n",
    "end_date = neph_zero_cal_data['Date'].iloc[-1]\n",
    "neph_zero_cal_data.set_index('Date', inplace=True)\n",
    "neph_zero_cal_data = neph_zero_cal_data.reindex(pd.date_range(start=pd.to_datetime(start_date, dayfirst=True), \n",
    "                                                              end=pd.to_datetime(end_date, dayfirst=True), \n",
    "                                                              freq='5min', inclusive='both'))\n",
    "neph_zero_cal_data.reset_index(inplace=True, names='Date')\n",
    "#neph_zero_cal_30min = resample_dataframe(neph_zero_cal_data, resample_interval='30 min', date_column='Date')\n",
    "\n",
    "\n",
    "start_date = neph_zero_cal['Date'].iloc[0]\n",
    "end_date = neph_zero_cal['Date'].iloc[-1]\n",
    "neph_zero_cal.set_index('Date', inplace=True)\n",
    "neph_zero_cal = neph_zero_cal.reindex(pd.date_range(start=pd.to_datetime(start_date, dayfirst=True), \n",
    "                                                    end=pd.to_datetime(end_date, dayfirst=True), \n",
    "                                                    freq='5min', inclusive='both'))\n",
    "neph_zero_cal.reset_index(inplace=True, names='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data correction\n",
    "Neph  C=a+b*Å 450Åblue:450/525blue/green, 525Ågreen:450/635 blue/red, 635Åred:525/635 green/red in paper (T.Muller 2011)\\\n",
    "for aurora no cut blue;a=1.455 b=-0,189 gree a=1,434 b=-0,176 red a=1,403 b=-0,156\\\n",
    "Tsca 450 nm (blue), 525 nm (green) and 635 nm (red) ,reading order is 1red 2green 3blue\\ \n",
    "sca 450 blue/ 635nm red\\\n",
    " \n",
    " a_b=1.455, b_b=-0.189\\\n",
    " a_g=1.434, b_g=-0.176\\\n",
    " a_r=1.403, b_r=-0.156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Truncation Correction WIP ###\n",
    "blue_light = '450nm'\n",
    "green_light = '525nm'\n",
    "red_light = '635nm'\n",
    "# Constants for blue, green, and red\n",
    "a_b = 1.455\n",
    "b_b = -0.189\n",
    "a_g = 1.434\n",
    "b_g = -0.176\n",
    "a_r = 1.403\n",
    "b_r = -0.156\n",
    "\n",
    "for column in neph_zero_cal_data.columns:\n",
    "    # Assuming scat_uncor is a NumPy array with at least 3 columns (blue, green, red)\n",
    "    # Example: scat_uncor = np.array([[val1, val2, val3], [val4, val5, val6], ...])\n",
    "\n",
    "    # Initialize an array for corrected scatter values\n",
    "    scat_cor = np.zeros((len(neph_zero_cal_data), 6))\n",
    "    if blue_light in column:\n",
    "        # Correction for blue (scat_uncor[:, 2] and scat_uncor[:, 1])\n",
    "        SAE_bg = np.zeros(len(neph_zero_cal_data))\n",
    "        c_bg = np.zeros(len(neph_zero_cal_data))\n",
    "        for k in range(len(neph_zero_cal_data)):\n",
    "            SAE_bg[k] = -np.log(neph_zero_cal_data[k, 2] / neph_zero_cal_data[k, 1]) / np.log(450 / 525)\n",
    "            c_bg[k] = a_b + b_b * SAE_bg[k]\n",
    "            scat_cor[k, 0] = neph_zero_cal_data[k, 0] * c_bg[k]  # First column (blue)\n",
    "    if green_light in column:\n",
    "        # Correction for green (scat_uncor[:, 2] and scat_uncor[:, 0])\n",
    "        SAE_br = np.zeros(len(neph_zero_cal_data))\n",
    "        c_br = np.zeros(len(neph_zero_cal_data))\n",
    "        for k in range(len(neph_zero_cal_data)):\n",
    "            SAE_br[k] = -np.log(neph_zero_cal_data[k, 2] / neph_zero_cal_data[k, 0]) / np.log(450 / 635)\n",
    "            c_br[k] = a_g + b_g * SAE_br[k]\n",
    "            scat_cor[k, 1] = neph_zero_cal_data[k, 1] * c_br[k]  # Second column (green)\n",
    "    if red_light in column:\n",
    "        # Correction for red (scat_uncor[:, 1] and scat_uncor[:, 0])\n",
    "        SAE_gr = np.zeros(len(neph_zero_cal_data))\n",
    "        c_gr = np.zeros(len(neph_zero_cal_data))\n",
    "        for k in range(len(neph_zero_cal_data)):\n",
    "            SAE_gr[k] = -np.log(neph_zero_cal_data[k, 1] / neph_zero_cal_data[k, 0]) / np.log(525 / 635)\n",
    "            c_gr[k] = a_r + b_r * SAE_gr[k]\n",
    "            scat_cor[k, 2] = neph_zero_cal_data[k, 2] * c_gr[k]  # Third column (red)\n",
    "\n",
    "        # scat_cor now contains the corrected scatter values for blue, green, and red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neph_test = pd.read_csv('neph_data_corrected/full_nephelometer_corrected_data.csv')\n",
    "neph_test['Date'] = pd.to_datetime(neph_test['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values = ['Final Scat450nm [Mm^-1]', 'Final Scat525nm [Mm^-1]', 'Final Scat635nm [Mm^-1]', 'Final BackScat450nm [Mm^-1]', 'Final BackScat525nm [Mm^-1]', 'Final BackScat635nm [Mm^-1]']\n",
    "df_slice = neph_test[neph_test['Date'] >= pd.to_datetime('2022/01/01 00:00')]\n",
    "\n",
    "for value in final_values:\n",
    "    param_name = value.split()\n",
    "    new_df = df_slice.loc[:, ['Date', value]].reset_index(drop=True)\n",
    "    data_flag = np.ones(len(new_df))\n",
    "    new_df[f'{param_name[1]} Flag'] = data_flag\n",
    "    # Rename columns dynamically\n",
    "    new_df = new_df.rename(columns={\n",
    "        \"Date\": \"x\",  # Rename 'Date' to 'x'\n",
    "        next(col for col in new_df.columns if 'flag' in col.lower()): \"Flag\",  # Find and rename the column containing 'flag'\n",
    "    })\n",
    "\n",
    "    # Find the remaining column and rename it to 'y'\n",
    "    remaining_column = [col for col in new_df.columns if col not in ['x', 'Flag']][0]\n",
    "    new_df = new_df.rename(columns={remaining_column: \"y\"})\n",
    "    new_df.to_csv(f'neph_data_corrected/neph_data_2022_2024/{value}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_plot(data, date_column, value_columns, save_fig=False, save_name=None):\n",
    "    min_y = []\n",
    "    max_y = []\n",
    "\n",
    "    # Ensure date column is in datetime format\n",
    "    data.loc[:, date_column] = pd.to_datetime(data[date_column])\n",
    "\n",
    "    # Determine the x-axis limits\n",
    "    start_date = data[date_column].min()\n",
    "    end_date = data[date_column].max()\n",
    "\n",
    "    # Calculate the extended x-range (2 weeks before and 2 weeks after the data)\n",
    "    x_start = start_date - pd.Timedelta(weeks=2)\n",
    "    x_end = end_date + pd.Timedelta(weeks=2)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Add a line plot for each value column\n",
    "    for value_column in value_columns:\n",
    "        plt.plot(data[date_column], data[value_column], label=value_column[10:-8])  # Remove the prefix/suffix from the label\n",
    "        min_y.append(data[value_column].min())\n",
    "        max_y.append(data[value_column].max())\n",
    "\n",
    "    # Setting the x-axis and y-axis limits based on your requested ranges\n",
    "    plt.xlim(pd.to_datetime('2022/01/01 00:00'), data[date_column].iloc[-1])  # Update x-axis range\n",
    "    y_min = min(min_y) * 0.95  # 5% below the minimum value\n",
    "    y_max = max(max_y) * 1.05  # 5% above the maximum value\n",
    "    plt.ylim(-2, 50)  # Use the fixed y-axis range as per your request (adjusted from your example)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title('Nephelometer Measurements Jan 2022 - Sep 2024')\n",
    "    plt.xlabel(f'{date_column} [YYYY-mm]')\n",
    "    plt.ylabel(r'Scattering [Mm$^{-1}$]')\n",
    "\n",
    "    # Format the x-axis to show dates properly\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Display legend\n",
    "    plt.legend(title='Wavelengths', loc='upper left')\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the interactive time series plot\n",
    "    neph_plot = create_time_series_plot(neph_test[neph_test['Date'] >= pd.to_datetime('2022/01/01 00:00')], \n",
    "                                        'Date', \n",
    "                                        ['Final Scat450nm [Mm^-1]', 'Final Scat525nm [Mm^-1]', 'Final Scat635nm [Mm^-1]'],\n",
    "                                        save_fig=True,\n",
    "                                        save_name='neph_measurements_jan_2022_sep_2024.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000  # Increase the chunk size (default is 0, which means no limit)\n",
    "\n",
    "def create_time_series_plot(data, date_column, value_columns, save_fig=False, save_name=None):\n",
    "    min_y = []\n",
    "    max_y = []\n",
    "\n",
    "    # Ensure date column is in datetime format\n",
    "    data.loc[:, date_column] = pd.to_datetime(data[date_column])\n",
    "\n",
    "    # Determine the x-axis limits\n",
    "    start_date = data[date_column].min()\n",
    "    end_date = data[date_column].max()\n",
    "\n",
    "    # Calculate the extended x-range (2 weeks before and 2 weeks after the data)\n",
    "    x_start = start_date - pd.Timedelta(weeks=2)\n",
    "    x_end = end_date + pd.Timedelta(weeks=2)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Add a line plot for each value column\n",
    "    for value_column in value_columns:\n",
    "        plt.plot(data[date_column], data[value_column], label=value_column[10:-8])  # Remove the prefix/suffix from the label\n",
    "        min_y.append(data[value_column].min())\n",
    "        max_y.append(data[value_column].max())\n",
    "\n",
    "    # Setting the x-axis and y-axis limits based on your requested ranges\n",
    "    plt.xlim(pd.to_datetime('2022/01/01 00:00'), data[date_column].iloc[-1])  # Update x-axis range\n",
    "    y_min = min(min_y) * 0.95  # 5% below the minimum value\n",
    "    y_max = max(max_y) * 1.05  # 5% above the maximum value\n",
    "    plt.ylim(-300, 400)  # Use the fixed y-axis range as per your request (adjusted from your example)\n",
    "\n",
    "    # Add title and labels\n",
    "    plt.title('Aethalometer Measurements Jan 2022 - Sep 2024')\n",
    "    plt.xlabel(f'{date_column} [YYYY-mm]')\n",
    "    plt.ylabel(r'BC [ng/m$^3$]')\n",
    "\n",
    "    # Format the x-axis to show dates properly\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Display legend\n",
    "    #plt.legend(title='Wavelengths', loc='upper left')\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    if save_fig:\n",
    "        plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the interactive time series plot\n",
    "    aeth_BC_5_min = resample_dataframe(aeth_BC_final, resample_interval='1min', date_column='Date')\n",
    "    aeth_plot = create_time_series_plot(aeth_BC_5_min[aeth_BC_5_min['Date'] >= pd.to_datetime('2022/01/01 00:00')], \n",
    "                                        'Date', \n",
    "                                        ['BC_final'],\n",
    "                                        save_fig=True,\n",
    "                                        save_name='aeth_measurements_jan_2022_sep_2024_1min.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neph_nan = neph_test.copy()\n",
    "neph_nan.set_index('Date', inplace=True)\n",
    "neph_nan.reindex(pd.date_range(pd.to_datetime(neph_test['Date'].iloc[0]), pd.to_datetime(neph_test['Date'].iloc[-1]), \n",
    "                               freq='5min'))\n",
    "neph_nan.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_plot(data, date_column, value_columns):\n",
    "    min_y = []\n",
    "    max_y = []\n",
    "    \n",
    "    # Ensure date column is in datetime format\n",
    "    data.loc[:, date_column] = pd.to_datetime(data[date_column])\n",
    "\n",
    "    # Determine the x-axis limits\n",
    "    start_date = data[date_column].min()\n",
    "    end_date = data[date_column].max()\n",
    "    x_start = start_date - pd.Timedelta(weeks=2)\n",
    "    x_end = end_date + pd.Timedelta(weeks=2)\n",
    "\n",
    "    # Create the plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add a line trace\n",
    "    for value_column in value_columns:\n",
    "        fig.add_trace(go.Scatter(x=data[date_column], y=data[value_column], mode='lines', name=value_column[10:-8]))\n",
    "        min_y.append(data[value_column].min()), max_y.append(data[value_column].max())\n",
    "\n",
    "    # Set x and y axis limits\n",
    "    fig.update_xaxes(range=[x_start, x_end], title_text=date_column)\n",
    "    \n",
    "    # Set y axis limits proportional to the values\n",
    "    y_min = min(min_y) * 0.95  # 5% below min value\n",
    "    y_max = max(max_y) * 1.05  # 5% above max value\n",
    "    fig.update_yaxes(range=[y_min, y_max], title_text=value_column)\n",
    "    fig.update_xaxes(range=[pd.to_datetime('2022/01/01 00:00'), data[date_column].iloc[-1]])\n",
    "\n",
    "    # Add title and layout adjustments\n",
    "    fig.update_layout(title='Nephelometer Measurements Jan 2022 - Sep 2024',\n",
    "                      template='plotly_white',\n",
    "                      xaxis_title=date_column,\n",
    "                      yaxis_title=r'Scattering [Mm$^{-1}$]',\n",
    "                      hovermode='x unified')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the interactive time series plot\n",
    "    neph_plot = create_time_series_plot(neph_nan[neph_nan['Date'] >= pd.to_datetime('2022/01/01 00:00')], 'Date', ['Final Scat450nm [Mm^-1]', 'Final Scat525nm [Mm^-1]', 'Final Scat635nm [Mm^-1]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make threshold to look for data with possible error. Threshold is set to a scattering coefficient of 100 Mm^-1\\\n",
    "Flag carryover from zero and span measurements with relevant flags\n",
    "\n",
    "### Flag types\n",
    "#### Flag 1\n",
    "(Value = 1)\n",
    "Good measurement, use as wanted.\n",
    "\n",
    "#### Flag 2\n",
    "Compromised measurement, use with caution, measurement might be perturbed by:\n",
    "- Flag 2 A (Value = 2)\n",
    "    - (Hyper-) Local source\n",
    "- Flag 2 B (Value = 3)\n",
    "    - Bad Zero or calibration\n",
    "\n",
    "#### Flag 3\n",
    "(Value = 4)\n",
    "Bad measurement, do not use!\n",
    "\n",
    "#### Flag 5\n",
    "Span and zero + carry over, do not use!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices_above_threshold(df, date_column, value_column, threshold):\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Create a boolean mask for values above the threshold\n",
    "    mask = df[value_column] >= threshold\n",
    "\n",
    "    # Filter the DataFrame based on the mask\n",
    "    above_threshold = df[mask]\n",
    "\n",
    "    if above_threshold.empty:\n",
    "        raise Exception('No values found above threshold.')\n",
    "\n",
    "    # Calculate the time differences\n",
    "    time_diffs = above_threshold[date_column].diff()\n",
    "\n",
    "    # Identify groups of consecutive values above the threshold\n",
    "    groups = (time_diffs >= pd.Timedelta('3 hour')).cumsum()\n",
    "\n",
    "    # Find tuples of indices for groups that last for more than 1 hour\n",
    "    index_tuples = []\n",
    "    for _, group in above_threshold.groupby(groups):\n",
    "        if len(group) > 0:\n",
    "            # Check if the group spans more than 1 hour, considering the averaging period\n",
    "            start_time = pd.to_datetime(group[date_column].iloc[0])\n",
    "            end_time = pd.to_datetime(group[date_column].iloc[-1])\n",
    "            if (end_time - start_time) >= pd.Timedelta('3 hour'):\n",
    "                index_tuples.append((group.index[0], group.index[-1]))\n",
    "\n",
    "    return index_tuples\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the threshold\n",
    "    threshold = 20\n",
    "    \n",
    "    # Find indices above threshold for more than 1 hour\n",
    "    indices = find_indices_above_threshold(neph_data.drop('Type', axis=1), 'Date', 'Scat525nm', threshold)\n",
    "    \n",
    "    # Display results\n",
    "    for idx in indices:\n",
    "        print(neph_data['Date'].iloc[idx[0]], neph_data['Date'].iloc[idx[1]+1], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = {'b1': 12, 'b4': 43, 'c': 534}\n",
    "b = (a.get('b2', None), a.get('b4', None))\n",
    "print(all(ele is None for ele in b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create the Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Create a simple Plotly figure\n",
    "fig = go.Figure(data=[go.Scatter(x=[1, 2, 3, 4], y=[1, 4, 9, 16])])\n",
    "\n",
    "# Define the layout of the Dash app\n",
    "app.layout = html.Div([\n",
    "    # Graph component to display the plot\n",
    "    dcc.Graph(id='graph', figure=fig),\n",
    "\n",
    "    # Text element to show live axis ranges\n",
    "    html.Div(id='axis-ranges', style={'font-size': '20px', 'margin-top': '20px'})\n",
    "])\n",
    "\n",
    "# Define a callback to update the live axis ranges\n",
    "@app.callback(\n",
    "    Output('axis-ranges', 'children'),\n",
    "    Input('graph', 'relayoutData')  # Listen to relayoutData property\n",
    ")\n",
    "def update_axis_ranges(relayout_data):\n",
    "\n",
    "    if relayout_data:\n",
    "        # Check if xaxis.range or yaxis.range exist\n",
    "        x_range = (relayout_data.get('xaxis.range[0]', None), relayout_data.get('xaxis.range[1]', None))\n",
    "        y_range = (relayout_data.get('yaxis.range[0]', None), relayout_data.get('yaxis.range[1]', None))\n",
    "        \n",
    "        # If axes are in autorange, they won't have a 'range' field, so handle that\n",
    "        if all(ele is not None for ele in x_range) and all(ele is not None for ele in y_range):\n",
    "            return f\"X-axis range: {x_range} | Y-axis range: {y_range}\"\n",
    "        elif (all(ele is not None for ele in x_range) and all(ele is None for ele in y_range)):\n",
    "            return f\"Y-axis is in autorange or unchanged. X-axis range: {x_range}\"\n",
    "        elif all(ele is None for ele in x_range) and all(ele is not None for ele in y_range):\n",
    "            return f\"X-axis is in autorange or unchanged. Y-axis range: {y_range}\"\n",
    "        elif all(ele is None for ele in x_range) and all(ele is None for ele in y_range):\n",
    "            return \"Axes are in autorange mode. Zoom or pan to change ranges.\"\n",
    "        \n",
    "    return \"Zoom or pan the graph to see axis ranges.\"\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Filter data based on the visible ranges\n",
    "    if all(ele is not None for ele in x_range) and all(ele is not None for ele in y_range):\n",
    "        filtered_df = df[(df['x'] >= x_range[0]) & (df['x'] <= x_range[1]) &\n",
    "                         (df['y'] >= y_range[0]) & (df['y'] <= y_range[1])]\n",
    "    \n",
    "    elif all(ele is None for ele in x_range) and all(ele is not None for ele in y_range):\n",
    "        filtered_df = df[(df['y'] >= y_range[0]) & (df['y'] <= y_range[1])]\n",
    "\n",
    "    elif all(ele is not None for ele in x_range) and all(ele is None for ele in y_range):\n",
    "        filtered_df = df[(df['x'] >= y_range[0]) & (df['x'] <= y_range[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fafafasf\n"
     ]
    }
   ],
   "source": [
    "relayout_data = {'selections': []}\n",
    "if relayout_data is None or next((True for axrng in relayout_data if 'autorange' in axrng), False):\n",
    "        if relayout_data is None or next((True for axrng in relayout_data if 'xaxis' in axrng), False) and \\\n",
    "         next((True for axrng in relayout_data if 'yaxis' in axrng), False):\n",
    "            print('both')\n",
    "\n",
    "        elif next((True for axrng in relayout_data if 'xaxis' in axrng), False) and \\\n",
    "         next((False for axrng in relayout_data if 'yaxis' in axrng), True):\n",
    "            print('y')\n",
    "\n",
    "        elif next((False for axrng in relayout_data if 'xaxis' in axrng), True) and \\\n",
    "         next((True for axrng in relayout_data if 'yaxis' in axrng), False):\n",
    "            print('x')\n",
    "        else:\n",
    "            print('fuck dig')\n",
    "else:\n",
    "     print('fafafasf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "relayout_data = {'yaxis.autorange': True, 'yaxis.showspikes': False}\n",
    "print(next((False for axrng in relayout_data if 'xaxis' in axrng), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\n",
    "    'x': np.linspace(0, 10, 100),\n",
    "    'y': np.sin(np.linspace(0, 10, 100)),\n",
    "    'value': np.random.randn(100)\n",
    "})\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv('bob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'x': [np.nan],\n",
    "    'y': [np.nan],\n",
    "    'Flag': [np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'x', 'id': 'x'}, {'name': 'y', 'id': 'y'}, {'name': 'Flag', 'id': 'Flag'}]\n"
     ]
    }
   ],
   "source": [
    "a = ['Flag']\n",
    "b = ['x']\n",
    "c = ['y']\n",
    "\n",
    "all_columns = [{'name': col, 'id': col} for col in  sorted(list(a + b + c), key=lambda col: df.columns.get_loc(col))]  # Define columns for the table\n",
    "\n",
    "print(all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
